{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557c5b63-ea11-491e-bd33-b7ab92b4f152",
   "metadata": {},
   "source": [
    "### Lecture 5.1 Regresssion\n",
    "#### Mean Squared Error(MSE)\n",
    "$$\n",
    "MSE(D,f) = \\frac1M\\sum^M_{i=1}(y^i-f(x^i))^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad584296-bc9b-41e4-a753-1bd963f45894",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "* Ordinary Least Squares(OLS)\n",
    "$$\n",
    "w^*,b^* = arg\\ min\\frac1M\\sum^M_{i=1}(y^i-w^Tx^i-b)^2\n",
    "$$\n",
    "Solving OLS, and we can get 2 equtions:\n",
    "* $(\\sum^M_{i=1}(x^i)^2)w + (\\sum^M_{i=1}x^i)b=\\sum^M_{i=1}y^ix^i$\n",
    "* $(\\sum^m_{I=1}x^i)w+Mb = \\sum^M_{i=1}y^i$\n",
    "\n",
    "#### General OLS solution\n",
    "add the bias element,making $w,x\\in R^{N+1}$\n",
    "$$\n",
    "w^* = arg\\ min \\frac1M\\sum^M_{i=1}(y^i-x(^i)^Tw)^2\n",
    "$$\n",
    "$$\n",
    "=arg \\ min \\frac1M (y-Xw)^T(y-Xw)\n",
    "$$\n",
    "\n",
    "Then we can get :\n",
    "$$\n",
    "w^* = (X^TX)^{-1}X^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc300c-82e7-4d9c-8d03-b3c25d973bba",
   "metadata": {},
   "source": [
    "#### Connection to Probabilistic Models\n",
    "Assumption: the output y is from a deterministic function with additive Gasussian noise:\n",
    "$$\n",
    "y = Xw+\\epsilon,where \\ p(\\epsilon;\\sigma^2) = N(\\epsilon;0,\\sigma^2I)\n",
    "$$\n",
    "This implies that:\n",
    "$$\n",
    "p(y|X;w,\\sigma^2) = p(y-Xw;\\sigma^2)=N(y-Xw;0\\sigma^2I)=N(y;Xw,\\sigma^2 I)\n",
    "$$\n",
    "\n",
    "maximize the log likelihood:\n",
    "$$\n",
    "agr\\ max\\ logp(y|X;w,\\sigma^2) ----> w^* = arg\\ min \\frac12 (y-Xw)^T(y-Xw)\n",
    "$$\n",
    "\n",
    "* what if $X^TX$ is non-invertible?\n",
    "  * if M< N+1 add more data or enforce regularization\n",
    "  * if M>= N+1 remove redundant features or enforce regularization\n",
    "\n",
    "* What if N is too large? ------ Use gradient descent\n",
    "  $w^{t+1} = w^t -\\alpha X^T(Xw^t-y)$\n",
    "\n",
    "* What if M is too large? ------ Use mini-batch gradient descent\n",
    "  $w^{t+1} = w^t -\\alpha \\sum_{x,y \\in \\beta} x(x^Tw^t-y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb4f4a0-f58c-4026-9cb6-1ec7e58aeeaf",
   "metadata": {},
   "source": [
    "\n",
    "### Regularized Linear Regression: Ridge and Lasson Regression\n",
    "why addiing regularization?\n",
    "(1) control the parameter space to aovid overfitting\n",
    "(2)obtain numerically more stable solution\n",
    "\n",
    "Add regularization term to OLS:\n",
    "$$\n",
    "min\\frac12||Xw-y||^2_2+\\frac\\alpha2||w||^2_2\n",
    "$$\n",
    "The first term is the data-fit term\n",
    "\n",
    "and the second term is the regularization term\n",
    "* $||w||^2_2 = \\sum w_j^2$ penalizes large weights\n",
    "* $\\alpha$ is the hyperparameter that controls the amount og shrinkage\n",
    "\n",
    "Has a probabilistic interpretation\n",
    "$$\n",
    "p(w|y,X;\\sigma^2,\\alpha) \\propto p(y|w,X;\\sigma^2)p(w;\\alpha)\n",
    "$$\n",
    "\n",
    "PS:\n",
    "(1)\n",
    "$$\n",
    "p(w;\\alpha) = N(w;\\mu,\\Sigma^{-1}) \n",
    "$$\n",
    "where $\\Sigma$ is a diagnal matrix and $\\mu$ often is 0\n",
    "(2) term $p(w)$ is prior and ther $p(y|w,X)$ is likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533280a4-30bc-4216-b8a0-a5c671bcf6e3",
   "metadata": {},
   "source": [
    "But why?\n",
    "Reason1:Smaller weights are more robust to perturbations of inpurt features\n",
    "Reason2: There are better chances to zero out some input features x that are reduncdant or uniformative, leading to more accurate prediction of y\n",
    "\n",
    "\n",
    "#### Ridge Regression \n",
    "$$\n",
    "min_w \\frac12 ||Xw-y||^2_2+\\frac\\alpha2||w||^2_2\n",
    "$$\n",
    "Has a closed-form solution\n",
    "$$\n",
    "w^*(X^TX+\\alpha I)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "#### Lasso Regression\n",
    "* with ridge regression, some weight are samll but still non-zero. There are less important, but somehow still necessary\n",
    "* To get better shrinkage to zero we can change the regularization term to encourage more weights to be zero\n",
    "\n",
    "$$\n",
    "min_w \\frac12||Xw-y||^2_2+\\alpha||w||_1\n",
    "$$\n",
    "regularization term:$||w||_1 = \\sum |w_j|$\n",
    "when a weight is close to zero, the regularization term weill force it to be equal to zero\n",
    "\n",
    "> it is a convex optimization problem with no closed-form solution\n",
    "\n",
    "\n",
    "Why shrinkage?\n",
    "Under the orthogonal design $X^TX=I$\n",
    "* Linear regression:$min_w\\frac12 ||Xw-y||^2_2$\n",
    "  $$\n",
    "  w_{LS} = (X^TX)^{-1}X^Ty = X^Ty\n",
    "  $$\n",
    "* Ridge regression: $min_w\\frac12 ||Xw-y||^2_2+\\frac\\alpha2 ||w||^2_2$\n",
    "  $$\n",
    "  w_{RR} = w_{LS}/(1+\\alpha)\n",
    "  $$\n",
    "* Lasso regression $min_w\\frac12 ||Xw-y||^2_2+\\frac\\alpha2 ||w||_1$\n",
    "  $$\n",
    "  w_{LR} = arg\\ min\\frac12 ||Xw-y||^2_2+\\alpha||w||_1 = sign(w_{LS})max(0,|w_{LS}|-\\alpha)\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ed7d5-b83e-4357-8c61-d9c7273adf1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
