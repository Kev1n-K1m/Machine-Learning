{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17729cbd-73e3-42d6-9104-fbbfcd97d8d1",
   "metadata": {},
   "source": [
    "## Lecutre 2.1 Naive Bayes and Linear Discriminant Analysis\n",
    "\n",
    "#### 2.1.1 The Bayes Optimal Classifier function\n",
    "\n",
    "\\begin{equation}\n",
    "f_B(\\pmb{x}) = arge\\ max_{c\\in\\{1,...,C\\}}p(y=c|x)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "=arg\\ max p(x|y=c)p(y=c)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "=arg\\ max log p(x|y=c)+logp(y=c)\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5479cba8-dc2a-477f-b9b5-d6040f65dbbc",
   "metadata": {},
   "source": [
    "#### 2.1.2 Naive Bayes\n",
    "\n",
    "Naive Bayes assuption\n",
    "* This form assumes that all of the feature dimensions are \n",
    "statistically independen\n",
    "* Allows us to model each dimension of the observation with a \n",
    "simple univariate distributio\n",
    "\n",
    "makes the general form for the classification function is:\n",
    "$$\n",
    "f_{NB}(x) = arg\\ max_{c\\in\\{1,...,C\\}}\\prod^N_{j=1}p(x_j|y=c)\n",
    "$$nt\n",
    "$\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019491ab-c446-4b21-ad89-e929fafb66ed",
   "metadata": {},
   "source": [
    "#### 2.1.3 Gaussian Classifier\n",
    "> Treat each dimension as an independent Gaussian and fit the Gaussian parameters with MLE\n",
    "Then view the posterior\n",
    "\n",
    "**geometry decision boundary**\n",
    "$$\n",
    "logp(y=1)+\\sum^2_{j=1}logp(x_j|y=1)=logp(y=2)+\\sum logp(x_j|y=2)\n",
    "$$\n",
    "quadratic function of($x_1,x_2$) with the form $\\sum^2_{j=1}(a_jx_j^2+b_jx_j)+c$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052bd40c-cc04-4c9c-8fc7-8e956d9b3378",
   "metadata": {},
   "source": [
    "#### 2.1.4 Naive Bayes Model For Boolean Vectors\n",
    "**Bernoulli distribution**\n",
    "$$\n",
    "with\\  indicator\\  funciton :p = \\varphi^{I[y=c]}* (1-\\varphi)^{I[y\\neq c]}\n",
    "$$\n",
    "\n",
    "**MLE parameters: $\\varphi_{j|c} = M_{j|c}/M_c$**\n",
    "\n",
    "$M_{j|c}$ is the number of training examples in Class c that contain feature $x_j$ \n",
    "\n",
    "$M_c$ is the number of training examples in Class c\n",
    "\n",
    "**Smoothed MLE**\n",
    "* parameter $\\varphi_{j|c} = (M_{j|c}+\\alpha)/(M_c+N\\alpha)$ in case of $\\varphi_{j|c}=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c88549-62fe-496d-a468-53fdde55b693",
   "metadata": {},
   "source": [
    "#### 2.1.4 Linear Discriminant Analysis\n",
    "Instead of a product of independent Gaussians in Naive Bayes, LDA assumes $p(x|y=c) = \\frac{1}{|(2\\pi)^N\\sigma|^{1/2}}exp(-\\frac12 (x-\\mu_c)^T\\sigma^{-1}(x-\\mu_c)$\n",
    "\n",
    "* classification function is：\n",
    "$$\n",
    "f_{LDA}(x) = arg\\ max_{c\\in\\{1,...,C\\}}p(y=c)p(x|y=c)\n",
    "\n",
    "* As with naive Bayes, LDA parameters are learned using MLE, \n",
    "which reduces to using sample estimate\n",
    "* class probabilities:$p(y=c)=\\frac1M \\sum^M_{i=1}I[y^{(1)}=c] = \\frac{M_c}{M}$\n",
    "* class mean: $\\mu_c = \\frac{\\sum^M_{i=1}I[y^{(i)}=c]x^{(i)}}{\\sum^M_{i=1}I[y^{(i)}=c]}$\n",
    "* shared covariance:$\\sigma=\\frac1M \\sum^M_{i=1}(x^i-\\mu_{y^i})(x^i-\\mu_{y^i})^T $\n",
    "\n",
    "**Decision boundary is actually linear in x**\n",
    "\n",
    "***NB versus LDA***\n",
    "* Storage:NB requires $O(N)$ parameters, whereas LDA requires $O(N^2)$\n",
    "  > NB: N $\\mu$ and N $\\sigma$ while LDA : N $\\mu$ and $\\frac{N*(N+1)}{2}$ in $\\sum$\n",
    "\n",
    "\n",
    "* Speed: The quadratic dependence on N makes LDA slower than \n",
    "NB during learning and classificatio\n",
    "* Interpretability: Both models have good interpretability since the \n",
    "parameters of p(x|y = c  correspond to class conditiona \r\n",
    "averag\n",
    "* Data: LDA will generally need more data than NB since it needs \n",
    "to estimate the O(N2) parameters in the shared covariance matrix2.1\r\n",
    "esns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b9255a0-4384-41fd-b6a7-c3641adfba4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里用的是高斯贝叶斯模型\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    " \n",
    "X = [[1,2],[3,4],[5,6],[7,8],[9,10]]\n",
    "y = [0,0,0,1,1]\n",
    " \n",
    "model = GaussianNB()\n",
    "model.fit(X,y)\n",
    " \n",
    "model.predict([[5,5]])\n",
    " \n",
    "# 输出结果\n",
    "# array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f9531-8d19-4cfa-98c3-05e18c779d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
