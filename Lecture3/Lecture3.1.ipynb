{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4431ff-8672-4ef7-8bda-909cb5783f0a",
   "metadata": {},
   "source": [
    "#### Lofistic Regression\n",
    "* Logistic regression is a probabilistic discriminative classifier\n",
    "* decision boundary :\n",
    "  $$\n",
    "  p(y=+1|x) = \\frac{1}{1+exp(-(w^Tx+b))}\n",
    "  $$\n",
    "  $$\n",
    "  log\\frac{p(y=+1|x)}{p(y=-1|x)} = w^Tx+b\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee75721f-a975-41f9-994f-1f66926106ca",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "Much of machine learning can be written as an optimization problem:\n",
    "$$\n",
    "min_\\theta\\ l(D;\\theta) = min_\\theta \\frac1M \\sum^M_{i=1}(x^i,y^i;\\theta)+r(\\theta)\n",
    "$$\n",
    "* $\\theta$ is a parameter vector to be optimized\n",
    "* $D = \\{(x^i,y^i)\\}$ is a finite training set\n",
    "* $r(\\dot)$ is a regularizer to prevent overfitting to D\n",
    "\n",
    "in logistic regression ($\\theta = {w,b}$)\n",
    "* $l(x^i,y^i;\\theta) = log(1+exp(-y^i(w^Tx^i+b)))$\n",
    "* $r(\\theta = \\frac1C W^TW)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a9175d-9d45-42b9-a07d-844ced9caaba",
   "metadata": {},
   "source": [
    "#### Convex optimizaiton and Nonconvex optimization\n",
    "###### Connvex Set\n",
    "Line segment : $x = \\alpha x^1+(1-\\alpha)x^2$ with $0<=\\alpha <=1$\n",
    "\n",
    "Convex set: contains line segment between any two points in the set\n",
    "\n",
    "Hyperplan: set of the form$\\{x|a^Tx=b\\}(a\\neq0)$\n",
    "\n",
    "Halfspace: set of the form $\\{x|a^Tx<=b\\}$ \n",
    "* Hyperplanes and halfspaces are convex\n",
    "\n",
    "##### Convex Function\n",
    "$f:R6N -> R$ is convex if dom(f) is a convex set and\n",
    "$$\n",
    "f(\\alpha x^1+(1-\\alpha)x^2) <= \\alpha f(x^1) + (1-\\alpha) f(x^2)\n",
    "$$\n",
    "$f$ is concave if -$f$ is convex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70095399-87a0-46c7-aa70-449976aa67b3",
   "metadata": {},
   "source": [
    "##### Convex\n",
    "* Affine: $ax+b\\  on\\  R$\n",
    "* Exponential:$e^{ax}$\n",
    "* Power:$x^a\\  on\\  R_{++}$\n",
    "* Power of absolute value:$|x|^p$ on R for p >= 1\n",
    "* Negative entropy: $xlogx$ on $R_++$\n",
    "\n",
    "##### Concave\n",
    "* Affine: $ax + b$ on R for any a,b in R\n",
    "* Powers: $x^a$ on $R_{++}$ for $0 <=a <=1$\n",
    "* Logarithm: $logx$ on $R_{++}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b43a9b-0d7c-4720-8ef0-497575cfa865",
   "metadata": {},
   "source": [
    "#### Example on $R^N$ and $R^{M*N}$\n",
    "1.$R^N$\n",
    "Affine function: $a^Tx+b$\n",
    "Norms:$||x||_p-------||x||_{\\inf} = max_j|x_j|$\n",
    "\n",
    "2.$R^{M*N}$\n",
    "Affine function :\n",
    "$$\n",
    "f(X) = \\sum^M \\sum^N A_{ij}X_{ij} + b = tr(A^TX) + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdca2e58-3917-4906-a3e0-43091b5c0e34",
   "metadata": {},
   "source": [
    "#### Properties of Convex Function\n",
    "Any line segment we draw between two points lies above the \n",
    "curv\n",
    "e\r\n",
    "Every local minimum is a global minimuhy?\r\n",
    "This is what makes convex optimization \n",
    "* asy\r\n",
    "It suffices to find a local minimum because we know it will be\r\n",
    "globaure 3.1\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e6c7a6-4ca9-4b15-9e1a-6393d79c24e3",
   "metadata": {},
   "source": [
    "1.Non-negative combinations of convex functions are convex\n",
    "$$\n",
    "h(x) = af(x) + bg(x)\n",
    "$$\n",
    "2.Affine scalings of convex functions are convex\n",
    "$$\n",
    "h(x) = f(Ax+b)\n",
    "$$\n",
    "3.Pointwise maximum of convex functions are convex\n",
    "\n",
    "4.ompositions of convex functions are NOT generally convex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be016017-40dd-4441-a0b5-4d9a0ded12df",
   "metadata": {},
   "source": [
    "#### First-Order Condition\n",
    "differentiable if the gradient\n",
    "\n",
    "$\\nabla f(x)$ exists at each $x \\in dom(f)$\n",
    "\n",
    "1st-order condition: differentiable f with convex domain is convex iff :\n",
    "$$\n",
    "f(y) >= f(x) + \\nabla f(x)^T(y-x) ,for \\ all \\ x,y \\in dom(f)\n",
    "$$ \n",
    "* This means that the Hessian matrix is positive semidefinite\n",
    "  $$\n",
    "  A>= 0 <=> anyx,x^TAx>=0\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007a95a-b438-4863-827f-f9783eb4dcbc",
   "metadata": {},
   "source": [
    "#### Second-Order Condition\n",
    "Hessian:$\\nabla$^2f(x)_{ij} exist at each $x \\in dom(f)$\n",
    "2st-order condition: differentiable f with convex domain is convex iff :\n",
    "$$\n",
    "\\nabla^2f(x)>=0\n",
    "$$\n",
    "\n",
    "#### Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4654b7-f8bc-4151-badf-886a65fb3e89",
   "metadata": {},
   "source": [
    "#### Gradient Descent \n",
    "$\\theta -< \\theta-\\alpha \\nabla l(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d48ac8-8398-4a5d-8412-31ded1a2e7fc",
   "metadata": {},
   "source": [
    "* $\\alpha$ is the learning rate\n",
    "* $\\nabla l(\\theta)$ is the gradient of $l(\\theta)$ evaluated at $\\theta$\n",
    "\n",
    "$$\n",
    "\\theta^{t+1} = \\theta^{t}-\\alpha \\nabla l(D;\\theta^{t}) = \\theta^{t} - \\alpha \\frac1M \\sum^M\\nabla l(x^i,y^i;\\theta)\n",
    "\n",
    "How to stop?\n",
    "\n",
    "$||\\theta^{t+1}-\\theta^i||_2 <= \\epsilon$\n",
    "\n",
    "$||\\nabla l(D;\\theta)||_2<=\\epsilon$\n",
    "\n",
    "* A negative gradient step can decrease the loss function\n",
    "  proof. Let $\\theta^0$ be any initial point. Using the first-order Taylor expansion for the next point\n",
    "  $$\n",
    "  l(\\theta^1) = l(\\theta^0)+\\nabla l(\\theta^0)^T(\\theta^1-\\theta^0)\n",
    "  $$\n",
    "  $$\n",
    "  =l(\\theta^0)+\\nabla l(\\theta^0)^T(-\\alpha \\nabla l(\\theta^0))\n",
    "  $$\n",
    "  $$\n",
    "  =l(\\theta^0)-\\alpha||\\nabla l(\\theta^0)||_2\n",
    "  $$\n",
    "\n",
    "  for sufficiently small $\\alpha$ -> satisfy the premise of the Taylor expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1461da-583e-45da-907e-9fba64e0d3f3",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "* the computational cost for GD at each iteration is O(M), which grows linearly with M\n",
    "* Stochastic gradient descent reduces the computational cost at each iteration to O(1)\n",
    "  $$\n",
    "  \\theta^{t+1} = \\theta^t - \\alpha \\nabla l(x^i,y^i;\\theta^t)\n",
    "  $$\n",
    "    Advantages:\r\n",
    "It is easy to fit into memory and computationally fast due to a\r\n",
    "single training sample being processed at each iteration\r\n",
    "The steps have oscillations, which may help the algorithm get out\r\n",
    "of bad local mi\n",
    "n    ima\r\n",
    "Disadvantages:\r\n",
    "The steps are noisy. The objective does not always decrease for\r\n",
    "each step and it may take longer to achieve convergence\r\n",
    "It loses the advantage of vectorized operations as it deals with\r\n",
    "only a single example aLecture 3.1\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a5d19-fdce-4ba6-825f-91215f381542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
