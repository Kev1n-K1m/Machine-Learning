{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31cd079-27c0-4f09-88f4-a4973e575a22",
   "metadata": {},
   "source": [
    "## Lec3.2 Support Vector Machines\n",
    "\n",
    "#### Maximum Margin pinciple\n",
    "define the distance between the eparating line and the closest point as the margin\n",
    "* the best sepatating line is the one that maximizes the margin\n",
    "\n",
    "magin: the plane $W^x + b = 0$\n",
    "$$\n",
    "d^i = \\frac{y^i(w^Tx^i+b)}{||w||_2} ---- geometric\\  margin\n",
    "$$\n",
    "\n",
    "$d = min(d^i)$ for all i\n",
    "\n",
    "margin solution is found by solving:\n",
    "$$\n",
    "max\\{\\frac1{||w||_2}min(w^Tx+b)\\}\n",
    "$$\n",
    "for the objective funciton is unchanged we can use:\n",
    "$$\n",
    "miny(W^Tx+b)=1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ab0cc-e9af-4592-8b9a-26a21771f139",
   "metadata": {},
   "source": [
    "So given a training set D optimize:\n",
    "\n",
    "$$\n",
    "min_{w,b} \\frac12 ||w||_2^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "subject to:y^i(w^Tx^i+b)>=1\n",
    "$$\n",
    "\n",
    "> The objective minimizes the inverse of the margin distance, i.e.,\n",
    "> maximizes the geometric margin\n",
    "> The inequality constraints ensure that all points are either on or\n",
    "> outside of the functional margin\n",
    "\n",
    "Prediction:\n",
    "$$\n",
    "y^* = sign(w^Tx^*+b)\n",
    "$$\n",
    "\n",
    "#### is Hard-margin Enough\n",
    "\n",
    "solution: allow some training samples to violate margin\n",
    "\n",
    "Define \"slack\" variable $\\eta_i$ >= 0\n",
    "* $\\eta_i$ = 0 means sample is outside of margin area(no slack)\n",
    "* $\\eta_i$ > 0 means sample is inside of margin area (slack)\n",
    "* $\\eta$ is regarded as a distance betweem the point and the$ w^Tx +b = +-1 $\n",
    "\n",
    "NOW:\n",
    "$$\n",
    "min_{w,b} \\frac12||w||_2^2 + C\\sum^M_{i=1}\\eta_i\n",
    "$$\n",
    "$$\n",
    "subject\\ to\\ y^i(w^Tx^i+b)>= 1-\\eta\n",
    "$$\n",
    "\n",
    "> here C as penalty for violating the margin. Small value means allow more violations(less penalty),Larger value meams don't allow violations(more penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fdb678-b22a-4e72-b693-82baef5c6529",
   "metadata": {},
   "source": [
    "in hte case of primal-form SVM with soft-margin, it is equivalent to minimizing the function:\n",
    "$$\n",
    "\\sum^M_{i=1}min(0,1-y^i(w^Tx^i+b))+\\frac1C||w||_2^2\n",
    "$$\n",
    "\n",
    "###### Hinge loss funciton $l_h(z) = max(0,1-z)$\n",
    "\n",
    "###### Zero-One loss\n",
    "$$\n",
    "l_{01}(z) = I[Z<=0]\n",
    "$$\n",
    "> the average zero-one loss over a data set is exactly the classification error rate\n",
    "> his is the loss function we’d like to minimize, but this generally \n",
    "isn’t computationally feasible, thus the need for surrogate los \r\n",
    "function computationaly infeasible: for the derivative of the function is a const -> make the descent invalid3.2\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a9ef3-7e54-4833-b6d0-ec397bbab9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
